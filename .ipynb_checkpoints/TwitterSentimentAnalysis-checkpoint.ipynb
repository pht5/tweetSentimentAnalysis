{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import oauth2\n",
    "import json\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "with open(\"twitter_auth.json\", 'r') as f:\n",
    "    auth_codes = json.load(f)\n",
    "    \n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "    \n",
    "def add_to_pickle_file(fname, new_data):\n",
    "    # Given a pickle filename, append to it the new data (in our case, a list of dicts)\n",
    "   \n",
    "    # Check that file is not empty, otherwise initialize\n",
    "    if os.path.getsize(fname) > 0:      \n",
    "        with open(fname, 'rb') as f:\n",
    "            unpickler = pickle.Unpickler(f);\n",
    "            data = unpickler.load()\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    print(\"Old length of file: {:d}\".format(len(data)))\n",
    "    data = data + new_data\n",
    "    print(\"New length of file: {:d}\".format(len(data)))\n",
    "    \n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def read_text(filename, comment=\";\"):\n",
    "    \"\"\" Given a filename, read in every line that is not commented\"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    out_list = []\n",
    "    \n",
    "    for line in f:\n",
    "        if line[0] != comment:\n",
    "            out_list.append(line.strip('\\n'))\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "pos_vocab = read_text(\"positive-words.txt\")\n",
    "neg_vocab = pos_vocab#read_text(\"negative-words.txt\")\n",
    "\n",
    "def read_words(filename):\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            out.append(line.strip('\\n'))\n",
    "    return out\n",
    "\n",
    "english_words = read_words(\"words_alpha.txt\")\n",
    "\n",
    "\n",
    "pickle_file = \"ben_labels.pckl\"\n",
    "# pickle_file = \"patrick_labels.pckl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fetch Tweets ##\n",
    "def oauth_req(url, http_method=\"GET\", post_body=b\"\", http_headers=None):\n",
    "    consumer = oauth2.Consumer(key=auth_codes[\"key\"], secret=auth_codes[\"secret\"])\n",
    "    token = oauth2.Token(key=\"\", secret=\"\")\n",
    "    client = oauth2.Client(consumer, token)\n",
    "    resp, content = client.request( url, method=http_method, body=post_body, headers=http_headers )\n",
    "    return content\n",
    "\n",
    "byteResponse = oauth_req('https://api.twitter.com/1.1/search/tweets.json?q=a&lang=en&count=100&tweet_mode=extended', \"GET\")\n",
    "tweets = json.loads(byteResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Label Tweets ##\n",
    "\n",
    "# Usage: \n",
    "# 0) Make empty file called \"patrick_labels.pckl\" in working directory\n",
    "# 1) Select correct pickle file\n",
    "# 2) Label tweets as appropriate\n",
    "# 3) When finished, type \"done\" into the emotion box to automatically have tweets uploaded to the pickle file. \n",
    "\n",
    "labeledTweets = []\n",
    "\n",
    "print(\"Total number of tweets to analyze in this session: {:d}\".format(len(tweets['statuses'])))\n",
    "\n",
    "# p - positive, n - negative, i - informational, h - humor, a - advertisement, o - none of the above\n",
    "for status in tweets['statuses']:\n",
    "    text = status['full_text']\n",
    "    # If this is a retweet, the full_text field gets cut off at 140 characters, \n",
    "    # so we have to get the whole text from the retweeted_status field\n",
    "    if text[0:4] == 'RT @':\n",
    "        text = status['retweeted_status']['full_text']\n",
    "    print(text)\n",
    "    emotion = input(\"What emotion is this? (type 'done' to save current progress to file) \")\n",
    "    print(\"\\n\")\n",
    "    if emotion.lower() == \"done\":\n",
    "        break\n",
    "    tweet = {\"status\": text, \"userId\": status['user']['id'], \"emotion\": emotion}\n",
    "    labeledTweets.append(tweet)\n",
    "\n",
    "add_to_pickle_file(pickle_file, labeledTweets)\n",
    "print(\"Added {:d} datapoints\".format(len(labeledTweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## transfer txt file to pickle file ##\n",
    "import ast\n",
    "\n",
    "f = open(\"patrickClassified.txt\",'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "dictString = \"\"\n",
    "inStatus = False\n",
    "tweets = []\n",
    "for i, line in enumerate(lines):\n",
    "    if \"\\'emotion\\'\" in line:\n",
    "        if \"[\" in line:\n",
    "            ind = line.index(\"[\") + 1\n",
    "            line = line[ind:]\n",
    "        dictString = dictString + line\n",
    "    elif \"\\'status\\'\" in line:\n",
    "        inStatus = True\n",
    "        dictString = dictString + line\n",
    "    elif \"\\'userId\\'\" in line:\n",
    "        inStatus = False\n",
    "        if \",\" in line:\n",
    "            ind = line.index(\",\")\n",
    "            line = line[:ind]\n",
    "        elif \"]\" in line:\n",
    "            ind = line.index(\"]\")\n",
    "            line = line[:ind]\n",
    "        dictString = dictString + line\n",
    "        tweet = ast.literal_eval(dictString.lstrip())\n",
    "        tweets.append(tweet)\n",
    "        dictString = \"\"\n",
    "    elif inStatus:\n",
    "        dictString = dictString + line\n",
    "\n",
    "#add_to_pickle_file(\"patrick_labels.pckl\", tweets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experimenting to see what features will work best ##\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Feature ideas: length of status, presence of url, number of @'s, number of #'s, presence/number of emojis, \n",
    "# number of numeric/special characters as opposed to alpha, wordnet to assess sentiment, number of alpha words that\n",
    "# are not real dictionary words, 'not' analysis?\n",
    "\n",
    "## Definitions ##\n",
    "def num_pos(status, pos_words=pos_vocab):\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    for w in word_tokenize(status):\n",
    "        if w in pos_words:\n",
    "            p = p + 1 \n",
    "            \n",
    "    return p\n",
    "\n",
    "def num_neg(status, neg_words=neg_vocab):\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    n = 0\n",
    "    for w in word_tokenize(status):\n",
    "        if w in neg_vocab:\n",
    "            n = n + 1 \n",
    "            \n",
    "    return n\n",
    "\n",
    "def count_emojis(status):\n",
    "    import regex as re\n",
    "    \n",
    "    emoji_regex = r'[\\uD83C-\\uDBFF\\uDC00-\\uDFFF]'\n",
    "    text_emoji_regex = r'[:;=][ -*]*?[\\[\\]\\(\\)DPpXx3/\\\\]' # Included some common regex expressions. '...*?...' \n",
    "    text_reverse_emoji_regex = r'[\\[\\]\\(\\)DPpxX/\\\\][ -*]*?[:;=]' # Included some common reverse regex expressions. \n",
    "    total_regex = r'(' + emoji_regex + r'|' + text_emoji_regex + r'|' + text_reverse_emoji_regex + r')'\n",
    "    rexp = re.compile(total_regex)\n",
    "    num_emojis = len(rexp.findall(status))\n",
    "    \n",
    "    return num_emojis\n",
    "\n",
    "def statusLength(status):\n",
    "    return len(status)\n",
    "\n",
    "def hasLink(status):\n",
    "    if \"https://\" in status:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def numAts(status):\n",
    "    return status.count(\"@\")\n",
    "\n",
    "def numHashtags(status):\n",
    "    return status.count(\"#\")\n",
    "\n",
    "features = [statusLength, hasLink, numAts, numHashtags]\n",
    "\n",
    "#Define data\n",
    "patrickData = load_pickle(\"patrick_labels.pckl\")\n",
    "patrickDatas = [patrickData[:270], patrickData[270:540], patrickData[540:]]\n",
    "numGroups = len(patrickDatas)\n",
    "#benData = load_pickle(\"ben_labels.pckl\")\n",
    "\n",
    "#Define categories\n",
    "allCategories = [\"p\",\"n\",\"h\",\"a\",\"i\",\"o\"]\n",
    "\n",
    "\n",
    "## Extract feature information from data ##\n",
    "featureDatas = []\n",
    "for i in range(0, numGroups):\n",
    "    allStatuses = {}\n",
    "    for category in allCategories:\n",
    "        allStatuses[category] = []\n",
    "    for tweet in patrickDatas[i]:\n",
    "        categories = tweet['emotion']\n",
    "        for category in categories:\n",
    "            allStatuses[category].append(tweet['status'])\n",
    "    \n",
    "    featureData = {}\n",
    "\n",
    "    for category in allCategories:\n",
    "        statuses = allStatuses[category]\n",
    "        numStatuses = len(statuses)\n",
    "        totals = {}\n",
    "        avgs = {}\n",
    "        #Initialize and calculate totals for each feature\n",
    "        for feature in features:\n",
    "            totals[feature] = 0\n",
    "        for status in statuses:\n",
    "            for feature in features:\n",
    "                totals[feature] += feature(status)\n",
    "        #Compute and storeaverages for each feature\n",
    "        for feature in features:\n",
    "            avgs[feature] = totals[feature]/numStatuses\n",
    "        featureData[category] = avgs\n",
    "        #for feature in features:\n",
    "            #print(totals[feature]/numStatuses)\n",
    "    featureDatas.append(featureData)\n",
    "\n",
    "    \n",
    "    \n",
    "## Plots to show results ##\n",
    "colors = ['b','r','g', 'c', 'm', 'k']\n",
    "for feature in features:\n",
    "    x_pos = np.arange(len(allCategories))\n",
    "    w = 0.9 / numGroups\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    #Total\n",
    "    y = []\n",
    "    for category in allCategories:\n",
    "        totalAvg = 0\n",
    "        for i in range(0, numGroups):\n",
    "            totalAvg += featureDatas[i][category][feature]/numGroups\n",
    "        y.append(totalAvg)\n",
    "    ax.bar([pos + w*(numGroups - 1)/2 for pos in x_pos], y,width=0.9,color='y',align='center')\n",
    "    \n",
    "    #For each group\n",
    "    for i in range(0, numGroups):\n",
    "        y = []\n",
    "        for category in allCategories:\n",
    "            y.append(featureDatas[i][category][feature])\n",
    "        ax.bar([pos + w*i for pos in x_pos], y,width=w,color=colors[i],align='center')\n",
    "\n",
    "    plt.xticks(x_pos, allCategories)\n",
    "    plt.title(feature.__name__)\n",
    "    plt.show()            totals[feature] += feature(status)\n",
    "    for feature in features:\n",
    "        print(totals[feature]/numStatuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
